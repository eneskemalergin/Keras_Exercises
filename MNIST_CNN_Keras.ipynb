{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer using Python Keras\n",
    "\n",
    "Author: Enes Kemal Ergin\n",
    "\n",
    "Code reference: [Keras Documentation](http://keras.io)\n",
    "\n",
    "ConvNet Intro: [My Explanations](http://eneskemalergin.github.io/2016/04/02/ConvNets_Explained/)\n",
    "\n",
    "In this notebook I will write the cnn model using keras to make a digit recognizer. My primary data source is MNIST data. A long the way I will put some definitions that I wasn't well aware of the meanings before writing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size?\n",
    "\n",
    "Batch size is basically the size of selected data in each epoch.\n",
    "\n",
    "> Epoch is one forward pass and one backward pass of all the training examples. But careful epoch and the iteration is not same. We can calculate number of iteration by using this example:\n",
    "\n",
    "> if we have 1000 training examples, and our batch size is 500, then it will take 2 iterations to complete 1 epoch. You can decide the number of epoch as well in the parameters of you model.\n",
    "\n",
    "If the batch size increases we will be needing more memory during the execution. So adjust your parameters carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_classes = 10\n",
    "nb_epoch = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\n",
      "15302656/15296311 [==============================] - 9s     \n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (60000, 1, 28, 28))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start building our model with keras building blocks way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential() # Our model will be sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small portion explain the model with some more detail:\n",
    "\n",
    "- Convolution2D:\n",
    "    - 4D tensor with shape: (Samples, channels, rows, columns)\n",
    "    - nb_filters: Number of convolutional filters\n",
    "    - nb_conv: Number of convolutional kernel used as row and columns\n",
    "    - input_shape: used to specify the shape of the input, just used in the first layer since it's the input shape\n",
    "  \n",
    "- Activation('relu'):\n",
    "    - activation function is a function that transforms a set of input signals into an output signal with specified way\n",
    "    - in here we used rectifier linera unit function\n",
    "    - f(x) = max(0,x)\n",
    "- MaxPooling2D:\n",
    "    - Pooling is a way of sub-sampling (reducing the dimension of the input)\n",
    "    - Max Pooling takes the max of each group and down sizes into nb_pool(2)\n",
    "- Dropout:\n",
    "    - Dropout is a way of regularization\n",
    "    - When random neurons are dropped out the network is forced to learn several independent representations of the patterns with identical input and output.\n",
    "    - 0.25 is a fraction of the input units to drop.\n",
    "    - Dropout(p): p must be in between 0 and 1    \n",
    "- Flatten:\n",
    "    - ?\n",
    "- Dense:\n",
    "    - ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='valid', input_shape=(1, img_rows, img_cols)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 1765s - loss: 0.3060 - acc: 0.9063 - val_loss: 0.0656 - val_acc: 0.9801\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 1746s - loss: 0.0986 - acc: 0.9706 - val_loss: 0.0435 - val_acc: 0.9853\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 1746s - loss: 0.0745 - acc: 0.9784 - val_loss: 0.0368 - val_acc: 0.9872\n",
      "Epoch 4/12\n",
      "27136/60000 [============>.................] - ETA: 932s - loss: 0.0616 - acc: 0.9804"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
